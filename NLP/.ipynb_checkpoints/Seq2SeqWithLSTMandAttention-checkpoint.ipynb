{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/?unapproved=1376&moderation-hash=5feb059786bee61a003bab66b69c7c18#comment-1376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English to French\n",
    "raw_data = (\n",
    "    ('What a ridiculous concept!', 'Quel concept ridicule !'),\n",
    "    ('Your idea is not entirely crazy.', \"Votre idée n'est pas complètement folle.\"),\n",
    "    (\"A man's worth lies in what he is.\", \"La valeur d'un homme réside dans ce qu'il est.\"),\n",
    "    ('What he did is very wrong.', \"Ce qu'il a fait est très mal.\"),\n",
    "    (\"All three of you need to do that.\", \"Vous avez besoin de faire cela, tous les trois.\"),\n",
    "    (\"Are you giving me another chance?\", \"Me donnez-vous une autre chance ?\"),\n",
    "    (\"Both Tom and Mary work as models.\", \"Tom et Mary travaillent tous les deux comme mannequins.\"),\n",
    "    (\"Can I have a few minutes, please?\", \"Puis-je avoir quelques minutes, je vous prie ?\"),\n",
    "    (\"Could you close the door, please?\", \"Pourriez-vous fermer la porte, s'il vous plaît ?\"),\n",
    "    (\"Did you plant pumpkins this year?\", \"Cette année, avez-vous planté des citrouilles ?\"),\n",
    "    (\"Do you ever study in the library?\", \"Est-ce que vous étudiez à la bibliothèque des fois ?\"),\n",
    "    (\"Don't be deceived by appearances.\", \"Ne vous laissez pas abuser par les apparences.\"),\n",
    "    (\"Excuse me. Can you speak English?\", \"Je vous prie de m'excuser ! Savez-vous parler anglais ?\"),\n",
    "    (\"Few people know the true meaning.\", \"Peu de gens savent ce que cela veut réellement dire.\"),\n",
    "    (\"Germany produced many scientists.\", \"L'Allemagne a produit beaucoup de scientifiques.\"),\n",
    "    (\"Guess whose birthday it is today.\", \"Devine de qui c'est l'anniversaire, aujourd'hui !\"),\n",
    "    (\"He acted like he owned the place.\", \"Il s'est comporté comme s'il possédait l'endroit.\"),\n",
    "    (\"Honesty will pay in the long run.\", \"L'honnêteté paye à la longue.\"),\n",
    "    (\"How do we know this isn't a trap?\", \"Comment savez-vous qu'il ne s'agit pas d'un piège ?\"),\n",
    "    (\"I can't believe you're giving up.\", \"Je n'arrive pas à croire que vous abandonniez.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = strip_accents(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_en, raw_data_fr = list(zip(*raw_data))\n",
    "raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
    "raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data_fr]\n",
    "raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data_fr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quel concept ridicule ! <end>',\n",
       " 'Votre idee n est pas completement folle . <end>',\n",
       " 'La valeur d un homme reside dans ce qu il est . <end>',\n",
       " 'Ce qu il a fait est tres mal . <end>',\n",
       " 'Vous avez besoin de faire cela tous les trois . <end>',\n",
       " 'Me donnez vous une autre chance ? <end>',\n",
       " 'Tom et Mary travaillent tous les deux comme mannequins . <end>',\n",
       " 'Puis je avoir quelques minutes je vous prie ? <end>',\n",
       " 'Pourriez vous fermer la porte s il vous plait ? <end>',\n",
       " 'Cette annee avez vous plante des citrouilles ? <end>',\n",
       " 'Est ce que vous etudiez a la bibliotheque des fois ? <end>',\n",
       " 'Ne vous laissez pas abuser par les apparences . <end>',\n",
       " 'Je vous prie de m excuser ! Savez vous parler anglais ? <end>',\n",
       " 'Peu de gens savent ce que cela veut reellement dire . <end>',\n",
       " 'L Allemagne a produit beaucoup de scientifiques . <end>',\n",
       " 'Devine de qui c est l anniversaire aujourd hui ! <end>',\n",
       " 'Il s est comporte comme s il possedait l endroit . <end>',\n",
       " 'L honnetete paye a la longue . <end>',\n",
       " 'Comment savez vous qu il ne s agit pas d un piege ? <end>',\n",
       " 'Je n arrive pas a croire que vous abandonniez . <end>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_fr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> Quel concept ridicule !',\n",
       " '<start> Votre idee n est pas completement folle .',\n",
       " '<start> La valeur d un homme reside dans ce qu il est .',\n",
       " '<start> Ce qu il a fait est tres mal .',\n",
       " '<start> Vous avez besoin de faire cela tous les trois .',\n",
       " '<start> Me donnez vous une autre chance ?',\n",
       " '<start> Tom et Mary travaillent tous les deux comme mannequins .',\n",
       " '<start> Puis je avoir quelques minutes je vous prie ?',\n",
       " '<start> Pourriez vous fermer la porte s il vous plait ?',\n",
       " '<start> Cette annee avez vous plante des citrouilles ?',\n",
       " '<start> Est ce que vous etudiez a la bibliotheque des fois ?',\n",
       " '<start> Ne vous laissez pas abuser par les apparences .',\n",
       " '<start> Je vous prie de m excuser ! Savez vous parler anglais ?',\n",
       " '<start> Peu de gens savent ce que cela veut reellement dire .',\n",
       " '<start> L Allemagne a produit beaucoup de scientifiques .',\n",
       " '<start> Devine de qui c est l anniversaire aujourd hui !',\n",
       " '<start> Il s est comporte comme s il possedait l endroit .',\n",
       " '<start> L honnetete paye a la longue .',\n",
       " '<start> Comment savez vous qu il ne s agit pas d un piege ?',\n",
       " '<start> Je n arrive pas a croire que vous abandonniez .']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_fr_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw strings to integer sequences\n",
    "#set filters to blank as we already took care of punctuation\n",
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(raw_data_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, 'you': 2, '?': 3, 'the': 4, 'a': 5, 'is': 6, 'he': 7, 'what': 8, 'in': 9, 'do': 10, 'can': 11, 't': 12, 'did': 13, 'giving': 14, 'me': 15, 'i': 16, 'few': 17, 'please': 18, 'this': 19, 'know': 20, 'ridiculous': 21, 'concept': 22, '!': 23, 'your': 24, 'idea': 25, 'not': 26, 'entirely': 27, 'crazy': 28, 'man': 29, 's': 30, 'worth': 31, 'lies': 32, 'very': 33, 'wrong': 34, 'all': 35, 'three': 36, 'of': 37, 'need': 38, 'to': 39, 'that': 40, 'are': 41, 'another': 42, 'chance': 43, 'both': 44, 'tom': 45, 'and': 46, 'mary': 47, 'work': 48, 'as': 49, 'models': 50, 'have': 51, 'minutes': 52, 'could': 53, 'close': 54, 'door': 55, 'plant': 56, 'pumpkins': 57, 'year': 58, 'ever': 59, 'study': 60, 'library': 61, 'don': 62, 'be': 63, 'deceived': 64, 'by': 65, 'appearances': 66, 'excuse': 67, 'speak': 68, 'english': 69, 'people': 70, 'true': 71, 'meaning': 72, 'germany': 73, 'produced': 74, 'many': 75, 'scientists': 76, 'guess': 77, 'whose': 78, 'birthday': 79, 'it': 80, 'today': 81, 'acted': 82, 'like': 83, 'owned': 84, 'place': 85, 'honesty': 86, 'will': 87, 'pay': 88, 'long': 89, 'run': 90, 'how': 91, 'we': 92, 'isn': 93, 'trap': 94, 'believe': 95, 're': 96, 'up': 97}\n"
     ]
    }
   ],
   "source": [
    "print(en_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw eng sentences to int sequences\n",
    "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 5, 21, 22, 23],\n",
       " [24, 25, 6, 26, 27, 28, 1],\n",
       " [5, 29, 30, 31, 32, 9, 8, 7, 6, 1],\n",
       " [8, 7, 13, 6, 33, 34, 1],\n",
       " [35, 36, 37, 2, 38, 39, 10, 40, 1],\n",
       " [41, 2, 14, 15, 42, 43, 3],\n",
       " [44, 45, 46, 47, 48, 49, 50, 1],\n",
       " [11, 16, 51, 5, 17, 52, 18, 3],\n",
       " [53, 2, 54, 4, 55, 18, 3],\n",
       " [13, 2, 56, 57, 19, 58, 3],\n",
       " [10, 2, 59, 60, 9, 4, 61, 3],\n",
       " [62, 12, 63, 64, 65, 66, 1],\n",
       " [67, 15, 1, 11, 2, 68, 69, 3],\n",
       " [17, 70, 20, 4, 71, 72, 1],\n",
       " [73, 74, 75, 76, 1],\n",
       " [77, 78, 79, 80, 6, 81, 1],\n",
       " [7, 82, 83, 7, 84, 4, 85, 1],\n",
       " [86, 87, 88, 9, 4, 89, 90, 1],\n",
       " [91, 10, 92, 20, 19, 93, 12, 5, 94, 3],\n",
       " [16, 11, 12, 95, 2, 96, 14, 97, 1]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  5, 21, 22, 23,  0,  0,  0,  0,  0],\n",
       "       [24, 25,  6, 26, 27, 28,  1,  0,  0,  0],\n",
       "       [ 5, 29, 30, 31, 32,  9,  8,  7,  6,  1]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add padding to have the same input length\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')\n",
    "data_en[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the same steps for French sentences\n",
    "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "fr_tokenizer.fit_on_texts(raw_data_fr_in)\n",
    "fr_tokenizer.fit_on_texts(raw_data_fr_out)\n",
    "\n",
    "data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n",
    "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,\n",
    "                                                           padding='post')\n",
    "\n",
    "data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n",
    "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,\n",
    "                                                            padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instanceof tf dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_en, data_fr_in, data_fr_out))\n",
    "dataset = dataset.shuffle(20).batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            lstm_size, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, sequence, states):\n",
    "        embed = self.embedding(sequence)\n",
    "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
    "\n",
    "        return output, state_h, state_c\n",
    "\n",
    "    def init_states(self, batch_size):\n",
    "        return (tf.zeros([batch_size, self.lstm_size]),\n",
    "                tf.zeros([batch_size, self.lstm_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_without_attention(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_size):\n",
    "        super(Decoder_without_attention, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            lstm_size, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, sequence, state):\n",
    "        embed = self.embedding(sequence)\n",
    "        lstm_out, state_h, state_c = self.lstm(embed, state)\n",
    "        logits = self.dense(lstm_out)\n",
    "\n",
    "        return logits, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sequences (1, 8)\n",
      "Encoder outputs (1, 8, 64)\n",
      "Encoder state_h (1, 64)\n",
      "Encoder state_c (1, 64)\n",
      "\n",
      "Destination vocab size 110\n",
      "Destination sequences (1, 7)\n",
      "Decoder outputs (1, 7, 110)\n",
      "Decoder state_h (1, 64)\n",
      "Decoder state_c (1, 64)\n"
     ]
    }
   ],
   "source": [
    "# Test our model\n",
    "EMBEDDING_SIZE = 32\n",
    "LSTM_SIZE = 64\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "encoder = Encoder(en_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
    "decoder = Decoder_without_attention(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "\n",
    "source_input = tf.constant([[1, 3, 5, 7, 2, 0, 0, 0]])\n",
    "initial_state = encoder.init_states(1)\n",
    "encoder_output, en_state_h, en_state_c = encoder(source_input, initial_state)\n",
    "\n",
    "target_input = tf.constant([[1, 4, 6, 9, 2, 0, 0]])\n",
    "decoder_output, de_state_h, de_state_c = decoder(target_input, (en_state_h, en_state_c))\n",
    "\n",
    "print('Source sequences', source_input.shape)\n",
    "print('Encoder outputs', encoder_output.shape)\n",
    "print('Encoder state_h', en_state_h.shape)\n",
    "print('Encoder state_c', en_state_c.shape)\n",
    "\n",
    "print('\\nDestination vocab size', fr_vocab_size)\n",
    "print('Destination sequences', target_input.shape)\n",
    "print('Decoder outputs', decoder_output.shape)\n",
    "print('Decoder state_h', de_state_h.shape)\n",
    "print('Decoder state_c', de_state_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a loss function\n",
    "#Since we padded zeros into the sequences, let’s not take those zeros into account when computing the loss:\n",
    "def loss_func(targets, logits):\n",
    "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training function in which we perform a forward pass followed by a backward pass.\n",
    "# use the @tf.function decorator to take advance of static graph computation (remove it when you want to debug)\n",
    "#Network’s computations need to be put under tf.GradientTape() to keep track of gradients\n",
    "@tf.function\n",
    "def train_step_without_attention(source_seq, target_seq_in, target_seq_out, en_initial_states):\n",
    "    #print(\"TRAIN_STEP START\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        en_outputs = encoder(source_seq, en_initial_states)\n",
    "        #print(\"EN_OUTPUTS OBJECT: \", en_outputs[1:])\n",
    "        en_states = en_outputs[1:]\n",
    "        de_states = en_states\n",
    "\n",
    "        de_outputs = decoder(target_seq_in, de_states)\n",
    "        logits = de_outputs[0]\n",
    "        loss = loss_func(target_seq_out, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    #print(\"TRAIN_STEP END\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let’s define a method for inference purpose. \n",
    "#What it does is basically a forward pass, but instead of target sequences, we will feed in the <start> token. \n",
    "#Every next time step will take the output of the last time step as input until \n",
    "#we hit the <end> token or the output sequence has exceed a specific length:\n",
    "def predict_without_attention():\n",
    "    #print(\"PREDICT START \")\n",
    "    test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
    "    print(test_source_text)\n",
    "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
    "    print(test_source_seq)\n",
    "\n",
    "    en_initial_states = encoder.init_states(1)\n",
    "    en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)\n",
    "\n",
    "    de_input = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
    "    de_state_h, de_state_c = en_outputs[1:]\n",
    "    out_words = []\n",
    "\n",
    "    while True:\n",
    "        de_output, de_state_h, de_state_c = decoder(\n",
    "            de_input, (de_state_h, de_state_c))\n",
    "        de_input = tf.argmax(de_output, -1)\n",
    "        out_words.append(fr_tokenizer.index_word[de_input.numpy()[0][0]])\n",
    "\n",
    "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
    "            break\n",
    "\n",
    "    print(' '.join(out_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 3.4235\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "s il il vous vous <end>\n",
      "Epoch 2 Loss 3.6857\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 3 Loss 3.5402\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 4 Loss 3.3937\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "vous vous vous <end>\n",
      "Epoch 5 Loss 3.8518\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 6 Loss 3.5430\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "vous vous vous <end>\n",
      "Epoch 7 Loss 3.4282\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous vous vous <end>\n",
      "Epoch 8 Loss 3.2189\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "vous vous vous <end>\n",
      "Epoch 9 Loss 2.7267\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous <end>\n",
      "Epoch 10 Loss 3.0996\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 11 Loss 3.3600\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 12 Loss 3.0703\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 13 Loss 2.8533\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 14 Loss 2.9312\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "vous vous vous vous <end>\n",
      "Epoch 15 Loss 2.8996\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 16 Loss 2.7178\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 17 Loss 2.8346\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 18 Loss 2.9634\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous vous vous vous <end>\n",
      "Epoch 19 Loss 2.5874\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "vous vous vous . <end>\n",
      "Epoch 20 Loss 3.1380\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "vous vous . <end>\n",
      "Epoch 21 Loss 3.0200\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "la . <end>\n",
      "Epoch 22 Loss 2.8617\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "vous vous vous vous vous vous <end>\n",
      "Epoch 23 Loss 2.6207\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "vous vous vous . <end>\n",
      "Epoch 24 Loss 2.4892\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "vous vous vous vous . <end>\n",
      "Epoch 25 Loss 2.5520\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous vous vous vous vous vous vous <end>\n",
      "Epoch 26 Loss 2.4823\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous vous vous vous vous vous <end>\n",
      "Epoch 27 Loss 2.9305\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "vous vous vous vous vous <end>\n",
      "Epoch 28 Loss 2.5384\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "vous vous vous vous vous vous <end>\n",
      "Epoch 29 Loss 2.7170\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "vous vous vous vous . <end>\n",
      "Epoch 30 Loss 2.8297\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "vous vous vous vous vous . <end>\n",
      "Epoch 31 Loss 2.3267\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "vous vous vous vous . <end>\n",
      "Epoch 32 Loss 2.9346\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "vous vous vous . <end>\n",
      "Epoch 33 Loss 2.6242\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous vous vous vous vous vous <end>\n",
      "Epoch 34 Loss 2.4584\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "vous vous vous la . <end>\n",
      "Epoch 35 Loss 2.6781\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "<end>\n",
      "Epoch 36 Loss 2.4984\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "il il il il <end>\n",
      "Epoch 37 Loss 2.4029\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous vous a la . <end>\n",
      "Epoch 38 Loss 2.6806\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous vous vous vous vous ? <end>\n",
      "Epoch 39 Loss 2.3582\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous vous vous a a vous . <end>\n",
      "Epoch 40 Loss 2.3338\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "il il il il <end>\n",
      "Epoch 41 Loss 2.4581\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous vous vous vous vous ? <end>\n",
      "Epoch 42 Loss 2.5931\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous vous vous a s vous . <end>\n",
      "Epoch 43 Loss 2.6112\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous vous a pas . <end>\n",
      "Epoch 44 Loss 2.2750\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "vous a la la <end>\n",
      "Epoch 45 Loss 2.5179\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous vous vous vous vous vous vous vous ? <end>\n",
      "Epoch 46 Loss 2.1326\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous vous vous vous vous ? <end>\n",
      "Epoch 47 Loss 2.2284\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous a a la . <end>\n",
      "Epoch 48 Loss 2.2770\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "ce a a la . <end>\n",
      "Epoch 49 Loss 2.3157\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "l de est est . <end>\n",
      "Epoch 50 Loss 1.9700\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "l de est est . <end>\n",
      "Epoch 51 Loss 2.5042\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "ce a a a . <end>\n",
      "Epoch 52 Loss 1.9949\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "votre a de est . <end>\n",
      "Epoch 53 Loss 1.9167\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "vous vous vous a a des ? <end>\n",
      "Epoch 54 Loss 2.1385\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "vous vous vous vous vous vous ? <end>\n",
      "Epoch 55 Loss 1.7086\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "l a de de est . <end>\n",
      "Epoch 56 Loss 2.1339\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous a a des ? <end>\n",
      "Epoch 57 Loss 2.3335\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "est a de . <end>\n",
      "Epoch 58 Loss 1.9280\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "<end>\n",
      "Epoch 59 Loss 1.9922\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "l paye de . <end>\n",
      "Epoch 60 Loss 1.7659\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "vous vous vous a la des ? <end>\n",
      "Epoch 61 Loss 1.9492\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "l a de de est . <end>\n",
      "Epoch 62 Loss 1.4238\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous vous vous vous vous vous ? <end>\n",
      "Epoch 63 Loss 1.9599\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous vous vous vous vous ? <end>\n",
      "Epoch 64 Loss 1.7576\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "l de de est est . <end>\n",
      "Epoch 65 Loss 1.7724\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "votre est est est . <end>\n",
      "Epoch 66 Loss 1.8761\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "est de est est les . <end>\n",
      "Epoch 67 Loss 1.7041\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous vous vous la vous vous ? <end>\n",
      "Epoch 68 Loss 2.0311\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous vous a des des ? <end>\n",
      "Epoch 69 Loss 1.2862\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous vous a a des des ? <end>\n",
      "Epoch 70 Loss 1.7441\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 71 Loss 1.5487\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "vous vous de s s il vous ? <end>\n",
      "Epoch 72 Loss 2.0587\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous vous vous la vous vous ? <end>\n",
      "Epoch 73 Loss 1.7106\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      ". <end>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Loss 1.3244\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous de a des les . <end>\n",
      "Epoch 75 Loss 1.6243\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "l a de de scientifiques . <end>\n",
      "Epoch 76 Loss 1.5545\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous vous vous vous la vous vous ? ? <end>\n",
      "Epoch 77 Loss 1.7872\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "l a a de longue . <end>\n",
      "Epoch 78 Loss 1.5357\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l a a de longue . <end>\n",
      "Epoch 79 Loss 1.7710\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "l paye de longue . <end>\n",
      "Epoch 80 Loss 1.5736\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "vous vous vous de la la vous ? <end>\n",
      "Epoch 81 Loss 1.5384\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous vous vous a la la des ? <end>\n",
      "Epoch 82 Loss 1.4676\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous vous a des des ? <end>\n",
      "Epoch 83 Loss 1.4834\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de est qui est est aujourd hui <end>\n",
      "Epoch 84 Loss 1.4363\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      ". <end>\n",
      "Epoch 85 Loss 1.6439\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous de la que vous ? <end>\n",
      "Epoch 86 Loss 1.1536\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous vous de s s s il vous ? <end>\n",
      "Epoch 87 Loss 1.1682\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "vous vous vous de s s s il vous ? <end>\n",
      "Epoch 88 Loss 1.7452\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "honnetete a a la longue . <end>\n",
      "Epoch 89 Loss 1.0425\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "votre n est pas completement . <end>\n",
      "Epoch 90 Loss 1.1074\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "vous vous vous la la la des ? <end>\n",
      "Epoch 91 Loss 1.3037\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous de de pas pas cela cela les . <end>\n",
      "Epoch 92 Loss 1.3039\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il il il s il il il ? <end>\n",
      "Epoch 93 Loss 1.4020\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "valeur homme homme homme reside ce il est . <end>\n",
      "Epoch 94 Loss 1.1312\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous vous vous des des ? <end>\n",
      "Epoch 95 Loss 0.9658\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous de de pas cela cela cela les . <end>\n",
      "Epoch 96 Loss 1.0457\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de qui est cela veut reellement . <end>\n",
      "Epoch 97 Loss 1.1531\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "l a a de longue . <end>\n",
      "Epoch 98 Loss 1.3802\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "valeur homme homme homme reside dans qu il est . <end>\n",
      "Epoch 99 Loss 1.3136\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de qui est c est anniversaire aujourd hui <end>\n",
      "Epoch 100 Loss 0.8422\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "je vous vous la la la des ? <end>\n",
      "Epoch 101 Loss 1.1113\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous de pas pas cela cela les . <end>\n",
      "Epoch 102 Loss 1.0623\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "l a a de longue . <end>\n",
      "Epoch 103 Loss 1.1967\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "de qui c est l . <end>\n",
      "Epoch 104 Loss 0.9716\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il est est . <end>\n",
      "Epoch 105 Loss 1.2296\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "l a a de longue . <end>\n",
      "Epoch 106 Loss 0.8475\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous de pas pas cela cela les . <end>\n",
      "Epoch 107 Loss 1.2089\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 108 Loss 0.9080\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de savent ce cela veut reellement . <end>\n",
      "Epoch 109 Loss 0.7604\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il est est . <end>\n",
      "Epoch 110 Loss 1.2588\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous de pas pas cela cela les . <end>\n",
      "Epoch 111 Loss 1.1104\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "l produit beaucoup scientifiques . <end>\n",
      "Epoch 112 Loss 1.0490\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "valeur valeur valeur homme homme reside dans ce qu il est . <end>\n",
      "Epoch 113 Loss 1.1939\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment vous vous comporte s s il il possedait endroit endroit . <end>\n",
      "Epoch 114 Loss 1.0176\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "ce a a a longue . <end>\n",
      "Epoch 115 Loss 0.8456\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom mary travaillent travaillent les deux mannequins . <end>\n",
      "Epoch 116 Loss 0.9393\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "je vous vous la la la des ? <end>\n",
      "Epoch 117 Loss 0.9727\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 118 Loss 0.5772\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "je vous pas a des . <end>\n",
      "Epoch 119 Loss 0.8652\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom mary travaillent travaillent les deux comme mannequins . <end>\n",
      "Epoch 120 Loss 0.6260\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "honnetete paye a la longue . <end>\n",
      "Epoch 121 Loss 0.9655\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "donnez vous une chance ? <end>\n",
      "Epoch 122 Loss 0.8161\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "je vous a la des ? <end>\n",
      "Epoch 123 Loss 0.9278\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il qu il s il il possedait endroit . <end>\n",
      "Epoch 124 Loss 0.6810\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous vous vous plante des des ? <end>\n",
      "Epoch 125 Loss 0.6605\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il est tres . <end>\n",
      "Epoch 126 Loss 0.5614\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "de qui c est veut dire . <end>\n",
      "Epoch 127 Loss 0.7670\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "vous vous de la que vous ? <end>\n",
      "Epoch 128 Loss 0.7955\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "la valeur valeur homme homme reside dans ce qu il il . <end>\n",
      "Epoch 129 Loss 0.6693\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous de de faire cela cela tous les . <end>\n",
      "Epoch 130 Loss 0.7019\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 131 Loss 0.9514\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent travaillent les deux comme mannequins . <end>\n",
      "Epoch 132 Loss 0.8803\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de qui c est l anniversaire aujourd hui ! <end>\n",
      "Epoch 133 Loss 0.6167\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "donnez vous une chance ? <end>\n",
      "Epoch 134 Loss 0.5292\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous avez vous plante des citrouilles ? <end>\n",
      "Epoch 135 Loss 0.8540\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous avez vous plante des citrouilles ? <end>\n",
      "Epoch 136 Loss 0.6363\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "donnez vous une chance ? <end>\n",
      "Epoch 137 Loss 0.6567\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de faire cela cela tous les . <end>\n",
      "Epoch 138 Loss 0.7395\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "je vous prie m excuser ! savez vous parler ? <end>\n",
      "Epoch 139 Loss 0.4532\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de faire cela cela les trois . <end>\n",
      "Epoch 140 Loss 0.4356\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 141 Loss 0.7012\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je vous arrive pas a croire que vous . <end>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142 Loss 0.6077\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il est tres . <end>\n",
      "Epoch 143 Loss 0.6462\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n est pas completement . <end>\n",
      "Epoch 144 Loss 0.5403\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 145 Loss 0.5560\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 146 Loss 0.5654\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 147 Loss 0.7049\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je vous avez vous plante des citrouilles ? <end>\n",
      "Epoch 148 Loss 0.7146\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 149 Loss 0.5567\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "idee n est pas completement . <end>\n",
      "Epoch 150 Loss 0.6018\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je n arrive pas a croire que vous abandonniez . <end>\n",
      "Epoch 151 Loss 0.5854\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 152 Loss 0.5751\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de faire cela tous les trois . <end>\n",
      "Epoch 153 Loss 0.5282\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "est vous que vous la la des des ? <end>\n",
      "Epoch 154 Loss 0.4217\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de savent ce que cela reellement dire . <end>\n",
      "Epoch 155 Loss 0.3382\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "n pas a a la . <end>\n",
      "Epoch 156 Loss 0.5704\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de savent ce que cela reellement dire . <end>\n",
      "Epoch 157 Loss 0.6399\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 158 Loss 0.4271\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 159 Loss 0.5051\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 160 Loss 0.5332\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "je vous prie de excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 161 Loss 0.4150\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "ne pas a a que . <end>\n",
      "Epoch 162 Loss 0.6247\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "donnez vous une autre <end>\n",
      "Epoch 163 Loss 0.3709\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "la valeur d un homme reside dans ce qu il est . <end>\n",
      "Epoch 164 Loss 0.4727\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "me vous une autre chance <end>\n",
      "Epoch 165 Loss 0.4506\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 166 Loss 0.4234\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte il s il possedait <end>\n",
      "Epoch 167 Loss 0.4525\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte il s il possedait <end>\n",
      "Epoch 168 Loss 0.4722\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de savent ce que cela reellement dire . <end>\n",
      "Epoch 169 Loss 0.3960\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 170 Loss 0.4670\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 171 Loss 0.4240\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "comment savez vous qu il s il possedait l endroit . <end>\n",
      "Epoch 172 Loss 0.4307\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il est tres un . <end>\n",
      "Epoch 173 Loss 0.2390\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "je vous prie de excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 174 Loss 0.2438\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "ne pas a croire que vous . <end>\n",
      "Epoch 175 Loss 0.3941\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 176 Loss 0.4275\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte comme s il possedait endroit . <end>\n",
      "Epoch 177 Loss 0.3216\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de savent ce que cela reellement dire . <end>\n",
      "Epoch 178 Loss 0.3624\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de qui est anniversaire aujourd hui <end>\n",
      "Epoch 179 Loss 0.3766\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte comme s il possedait endroit . <end>\n",
      "Epoch 180 Loss 0.4248\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 181 Loss 0.3109\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 182 Loss 0.2840\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment savez vous qu il s il agit pas d un piege ? <end>\n",
      "Epoch 183 Loss 0.2924\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je n arrive pas a croire que vous abandonniez . <end>\n",
      "Epoch 184 Loss 0.3842\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de qui c est l anniversaire aujourd hui ! <end>\n",
      "Epoch 185 Loss 0.3723\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je n arrive pas a croire que vous abandonniez . <end>\n",
      "Epoch 186 Loss 0.3747\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "cette avez vous plante des citrouilles ? <end>\n",
      "Epoch 187 Loss 0.3390\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je n arrive pas a croire que vous abandonniez . <end>\n",
      "Epoch 188 Loss 0.2857\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l paye a la longue . <end>\n",
      "Epoch 189 Loss 0.2379\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de de savent ce que cela reellement dire . <end>\n",
      "Epoch 190 Loss 0.2550\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "cette avez vous plante des citrouilles ? <end>\n",
      "Epoch 191 Loss 0.2093\n",
      "I can t believe you re giving up .\n",
      "[[16, 11, 12, 95, 2, 96, 14, 97, 1]]\n",
      "je n arrive pas a croire que vous abandonniez . <end>\n",
      "Epoch 192 Loss 0.3342\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 193 Loss 0.2867\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 194 Loss 0.3457\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "ne vous pas a la . <end>\n",
      "Epoch 195 Loss 0.3917\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte comme s il possedait endroit . <end>\n",
      "Epoch 196 Loss 0.3365\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte comme s il possedait endroit . <end>\n",
      "Epoch 197 Loss 0.1873\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l honnetete a la longue . <end>\n",
      "Epoch 198 Loss 0.3009\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "de qui c est l anniversaire aujourd hui ! <end>\n",
      "Epoch 199 Loss 0.2713\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez de faire cela tous les trois . <end>\n",
      "Epoch 200 Loss 0.2936\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "ne vous pas a la . <end>\n",
      "Epoch 201 Loss 0.2225\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "ne vous pas a la . <end>\n",
      "Epoch 202 Loss 0.2465\n",
      "Honesty will pay in the long run .\n",
      "[[86, 87, 88, 9, 4, 89, 90, 1]]\n",
      "l honnetete a la longue . <end>\n",
      "Epoch 203 Loss 0.2979\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "me vous une autre chance ? <end>\n",
      "Epoch 204 Loss 0.2817\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "je avoir quelques minutes je vous prie ? <end>\n",
      "Epoch 205 Loss 0.2759\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "ne vous pas a la . <end>\n",
      "Epoch 206 Loss 0.2556\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de qui est anniversaire aujourd hui ! <end>\n",
      "Epoch 207 Loss 0.2948\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "pourriez vous la porte s il vous ? <end>\n",
      "Epoch 208 Loss 0.3213\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de qui c est l anniversaire aujourd hui ! <end>\n",
      "Epoch 209 Loss 0.2531\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "votre n est pas completement folle . <end>\n",
      "Epoch 210 Loss 0.2040\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il est tres un . <end>\n",
      "Epoch 211 Loss 0.2953\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez besoin de faire cela tous les trois . <end>\n",
      "Epoch 212 Loss 0.2453\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "comment savez vous qu il ne s agit pas d piege ? <end>\n",
      "Epoch 213 Loss 0.1715\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "pourriez vous la porte s il vous ? <end>\n",
      "Epoch 214 Loss 0.1991\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de qui est anniversaire aujourd hui ! <end>\n",
      "Epoch 215 Loss 0.2318\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "comment savez vous qu il ne s agit pas d piege ? <end>\n",
      "Epoch 216 Loss 0.2148\n",
      "A man s worth lies in what he is .\n",
      "[[5, 29, 30, 31, 32, 9, 8, 7, 6, 1]]\n",
      "la valeur d un homme reside dans ce qu il est . <end>\n",
      "Epoch 217 Loss 0.2050\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "je vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 218 Loss 0.2019\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de qui est anniversaire aujourd hui ! <end>\n",
      "Epoch 219 Loss 0.1823\n",
      "Germany produced many scientists .\n",
      "[[73, 74, 75, 76, 1]]\n",
      "de qui est anniversaire aujourd hui ! <end>\n",
      "Epoch 220 Loss 0.1594\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "est ce que vous a la bibliotheque des fois ? <end>\n",
      "Epoch 221 Loss 0.2005\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "comment savez vous qu il ne s agit pas d piege ? <end>\n",
      "Epoch 222 Loss 0.2028\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "je vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 223 Loss 0.1895\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "devine de qui c est l anniversaire aujourd hui ! <end>\n",
      "Epoch 224 Loss 0.2236\n",
      "What he did is very wrong .\n",
      "[[8, 7, 13, 6, 33, 34, 1]]\n",
      "qu il est tres un . <end>\n",
      "Epoch 225 Loss 0.1493\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de gens savent ce que cela veut dire . <end>\n",
      "Epoch 226 Loss 0.1535\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "me vous une autre chance ? <end>\n",
      "Epoch 227 Loss 0.1830\n",
      "All three of you need to do that .\n",
      "[[35, 36, 37, 2, 38, 39, 10, 40, 1]]\n",
      "vous avez besoin de faire cela tous les trois . <end>\n",
      "Epoch 228 Loss 0.1053\n",
      "Can I have a few minutes please ?\n",
      "[[11, 16, 51, 5, 17, 52, 18, 3]]\n",
      "puis je avoir quelques minutes je vous prie ? <end>\n",
      "Epoch 229 Loss 0.2039\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment savez vous qu il ne s agit pas d un piege ? <end>\n",
      "Epoch 230 Loss 0.1927\n",
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "comment savez vous qu il ne s agit pas d un piege ? <end>\n",
      "Epoch 231 Loss 0.1502\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "me vous une autre chance ? <end>\n",
      "Epoch 232 Loss 0.2092\n",
      "He acted like he owned the place .\n",
      "[[7, 82, 83, 7, 84, 4, 85, 1]]\n",
      "il s est comporte comme s il possedait endroit . <end>\n",
      "Epoch 233 Loss 0.1371\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "est ce que vous a la bibliotheque des fois ? <end>\n",
      "Epoch 234 Loss 0.2307\n",
      "Few people know the true meaning .\n",
      "[[17, 70, 20, 4, 71, 72, 1]]\n",
      "de gens savent ce que cela veut dire . <end>\n",
      "Epoch 235 Loss 0.1502\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "est ce que vous a la bibliotheque des fois ? <end>\n",
      "Epoch 236 Loss 0.2214\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 237 Loss 0.1937\n",
      "Do you ever study in the library ?\n",
      "[[10, 2, 59, 60, 9, 4, 61, 3]]\n",
      "est ce que vous a la bibliotheque des fois ? <end>\n",
      "Epoch 238 Loss 0.1488\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 239 Loss 0.2364\n",
      "Are you giving me another chance ?\n",
      "[[41, 2, 14, 15, 42, 43, 3]]\n",
      "me vous une autre chance ? <end>\n",
      "Epoch 240 Loss 0.1145\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "votre idee est pas completement folle . <end>\n",
      "Epoch 241 Loss 0.1832\n",
      "Could you close the door please ?\n",
      "[[53, 2, 54, 4, 55, 18, 3]]\n",
      "pourriez vous la porte s il vous plait ? <end>\n",
      "Epoch 242 Loss 0.1588\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "votre idee est pas completement folle . <end>\n",
      "Epoch 243 Loss 0.1538\n",
      "Both Tom and Mary work as models .\n",
      "[[44, 45, 46, 47, 48, 49, 50, 1]]\n",
      "tom et mary travaillent tous les deux comme mannequins . <end>\n",
      "Epoch 244 Loss 0.1160\n",
      "Did you plant pumpkins this year ?\n",
      "[[13, 2, 56, 57, 19, 58, 3]]\n",
      "cette annee avez plante des citrouilles ? <end>\n",
      "Epoch 245 Loss 0.1270\n",
      "Don t be deceived by appearances .\n",
      "[[62, 12, 63, 64, 65, 66, 1]]\n",
      "ne vous pas a la <end>\n",
      "Epoch 246 Loss 0.1324\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "je vous prie de m excuser ! savez vous parler anglais ? <end>\n",
      "Epoch 247 Loss 0.1328\n",
      "What a ridiculous concept !\n",
      "[[8, 5, 21, 22, 23]]\n",
      "quel <end>\n",
      "Epoch 248 Loss 0.1434\n",
      "Your idea is not entirely crazy .\n",
      "[[24, 25, 6, 26, 27, 28, 1]]\n",
      "votre idee est pas completement folle . <end>\n",
      "Epoch 249 Loss 0.1203\n",
      "Guess whose birthday it is today .\n",
      "[[77, 78, 79, 80, 6, 81, 1]]\n",
      "devine de qui c est l anniversaire aujourd hui ! <end>\n",
      "Epoch 250 Loss 0.1608\n",
      "Excuse me . Can you speak English ?\n",
      "[[67, 15, 1, 11, 2, 68, 69, 3]]\n",
      "je vous prie de m excuser ! savez vous parler anglais ? <end>\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "NUM_EPOCHS = 250\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    en_initial_states = encoder.init_states(BATCH_SIZE)\n",
    "\n",
    "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "        loss = train_step_without_attention(source_seq, target_seq_in,\n",
    "                          target_seq_out, en_initial_states)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(e + 1, loss.numpy()))\n",
    "    \n",
    "    try:\n",
    "        predict_without_attention()\n",
    "    except Exception:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, rnn_size, attention_func):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.attention_func = attention_func\n",
    "\n",
    "        if attention_func not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(\n",
    "                'Unknown attention score function! Must be either dot, general or concat.')\n",
    "\n",
    "        if attention_func == 'general':\n",
    "            # General score function\n",
    "            self.wa = tf.keras.layers.Dense(rnn_size)\n",
    "        elif attention_func == 'concat':\n",
    "            # Concat score function\n",
    "            self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
    "            self.va = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, decoder_output, encoder_output):\n",
    "        if self.attention_func == 'dot':\n",
    "            # Dot score function: decoder_output (dot) encoder_output\n",
    "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
    "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
    "            # => score has shape: (batch_size, 1, max_len)\n",
    "            score = tf.matmul(decoder_output, encoder_output, transpose_b=True)\n",
    "        elif self.attention_func == 'general':\n",
    "            # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
    "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
    "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
    "            # => score has shape: (batch_size, 1, max_len)\n",
    "            score = tf.matmul(decoder_output, self.wa(\n",
    "                encoder_output), transpose_b=True)\n",
    "        elif self.attention_func == 'concat':\n",
    "            # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
    "            # Decoder output must be broadcasted to encoder output's shape first\n",
    "            decoder_output = tf.tile(\n",
    "                decoder_output, [1, encoder_output.shape[1], 1])\n",
    "\n",
    "            # Concat => Wa => va\n",
    "            # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
    "            score = self.va(\n",
    "                self.wa(tf.concat((decoder_output, encoder_output), axis=-1)))\n",
    "\n",
    "            # Transpose score vector to have the same shape as other two above\n",
    "            # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
    "            score = tf.transpose(score, [0, 2, 1])\n",
    "\n",
    "        # alignment a_t = softmax(score)\n",
    "        alignment = tf.nn.softmax(score, axis=2)\n",
    "\n",
    "        # context vector c_t is the weighted average sum of encoder output\n",
    "        context = tf.matmul(alignment, encoder_output)\n",
    "\n",
    "        return context, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewrite the decoder\n",
    "#At each time step t, we will concatenate the context vector and the current output (of the RNN unit) to \n",
    "#form a new output vector. We then continue as normal: convert that vector to vocabulary space for the final\n",
    "#output.\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, rnn_size, attention_func):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention = LuongAttention(rnn_size, attention_func)\n",
    "        self.rnn_size = rnn_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            rnn_size, return_sequences=True, return_state=True)\n",
    "        self.wc = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
    "        self.ws = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, sequence, state, encoder_output):\n",
    "        # Remember that the input to the decoder\n",
    "        # is now a batch of one-word sequences,\n",
    "        # which means that its shape is (batch_size, 1)\n",
    "        embed = self.embedding(sequence)\n",
    "\n",
    "        # Therefore, the lstm_out has shape (batch_size, 1, rnn_size)\n",
    "        lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n",
    "\n",
    "        # Use self.attention to compute the context and alignment vectors\n",
    "        # context vector's shape: (batch_size, 1, rnn_size)\n",
    "        # alignment vector's shape: (batch_size, 1, source_length)\n",
    "        context, alignment = self.attention(lstm_out, encoder_output)\n",
    "\n",
    "        # Combine the context vector and the LSTM output\n",
    "        # Before combined, both have shape of (batch_size, 1, rnn_size),\n",
    "        # so let's squeeze the axis 1 first\n",
    "        # After combined, it will have shape of (batch_size, 2 * rnn_size)\n",
    "        lstm_out = tf.concat(\n",
    "            [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
    "\n",
    "        # lstm_out now has shape (batch_size, rnn_size)\n",
    "        lstm_out = self.wc(lstm_out)\n",
    "\n",
    "        # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n",
    "        logits = self.ws(lstm_out)\n",
    "\n",
    "        return logits, state_h, state_c, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify the train_step function. Since we are dealing with each time step at a time on the decoder’s side, we will need to explicitly create a loop for that:\n",
    "@tf.function\n",
    "def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        en_outputs = encoder(source_seq, en_initial_states)\n",
    "        en_states = en_outputs[1:]\n",
    "        de_state_h, de_state_c = en_states\n",
    "        \n",
    "        # We need to create a loop to iterate through the target sequences\n",
    "        for i in range(target_seq_out.shape[1]):\n",
    "            # Input to the decoder must have shape of (batch_size, length)\n",
    "            # so we need to expand one dimension\n",
    "            decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
    "            logit, de_state_h, de_state_c, _ = decoder(\n",
    "                decoder_in, *(de_state_h, de_state_c), en_outputs[0])\n",
    "            \n",
    "            # The loss is now accumulated through the whole batch\n",
    "            loss += loss_func(target_seq_out[:, i], logit)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss / target_seq_out.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify predict function. We also need get the source sequence, the translated sequence and the alignment vector for visualization purpose:\n",
    "def predict(test_source_text=None):\n",
    "    if test_source_text is None:\n",
    "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
    "    print(test_source_text)\n",
    "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
    "    print(test_source_seq)\n",
    "\n",
    "    en_initial_states = encoder.init_states(1)\n",
    "    en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)\n",
    "\n",
    "    de_input = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
    "    de_state_h, de_state_c = en_outputs[1:]\n",
    "    de_state =[de_state_h, de_state_c]\n",
    "    \n",
    "    out_words = []\n",
    "    alignments = []\n",
    "\n",
    "    while True:\n",
    "        de_output, de_state_h, de_state_c, alignment = decoder(de_input, de_state , en_outputs[0])\n",
    "        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
    "        out_words.append(fr_tokenizer.index_word[de_input.numpy()[0][0]])\n",
    "        \n",
    "        alignments.append(alignment.numpy())\n",
    "\n",
    "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
    "            break\n",
    "\n",
    "    print(' '.join(out_words))\n",
    "    return np.array(alignments), test_source_text.split(' '), out_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do we know this isn t a trap ?\n",
      "[[91, 10, 92, 20, 19, 93, 12, 5, 94, 3]]\n",
      "s donnez s donnez s donnez s donnez s donnez s donnez s donnez s donnez s donnez s donnez\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    <ipython-input-19-daac7785e3e5>:15 train_step  *\n        logit, de_state_h, de_state_c, _ = decoder(\n    /usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:778 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n\n    TypeError: tf__call() takes 4 positional arguments but 5 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f7eac7ebd8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         loss = train_step(source_seq, target_seq_in,\n\u001b[0;32m---> 42\u001b[0;31m                           target_seq_out, en_initial_states)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {} Loss {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-19-daac7785e3e5>:15 train_step  *\n        logit, de_state_h, de_state_c, _ = decoder(\n    /usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:778 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n\n    TypeError: tf__call() takes 4 positional arguments but 5 were given\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "EMBEDDING_SIZE = 32\n",
    "RNN_SIZE = 512\n",
    "BATCH_SIZE = 5\n",
    "ATTENTION_FUNC = 'concat'\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
    "\n",
    "encoder = Encoder(en_vocab_size, EMBEDDING_SIZE, RNN_SIZE)\n",
    "decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, RNN_SIZE, ATTENTION_FUNC)\n",
    "# These lines can be used for debugging purpose\n",
    "# Or can be seen as a way to build the models\n",
    "initial_state = encoder.init_states(1)\n",
    "encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n",
    "decoder_outputs = decoder(tf.constant(\n",
    "    [[1]]), encoder_outputs[1:], encoder_outputs[0])\n",
    "\n",
    "\n",
    "def loss_func(targets, logits):\n",
    "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    en_initial_states = encoder.init_states(BATCH_SIZE)\n",
    "    \n",
    "    predict()\n",
    "\n",
    "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "        loss = train_step(source_seq, target_seq_in,\n",
    "                          target_seq_out, en_initial_states)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(e + 1, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
